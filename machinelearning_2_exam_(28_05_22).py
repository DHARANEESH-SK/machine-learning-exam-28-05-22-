# -*- coding: utf-8 -*-
"""machinelearning 2 exam (28/05/22).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17qiDb0cE35hvQZlIic7G_lTuHdOnSuqY
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df= pd.read_csv('/content/credit_card.csv')

df

"""Q1.
What does the primary analysis of several categorical features reveal?

* Categorical Values in dataset:
  
CUST_ID : It displays that there were unique number given to each of the customer. 

BALANCE_FREQUENCY : It shows how often the balance is getting updated in 0s and 1s. 

ONE_OFF_PURCHASE_FREQUENCY : It displays the information about how often the users use their credit cards for purchasing.

PURCHASE_FREQUENCY : It displays the information about how the users purchasing their products frequently . It displays the values in 0s and 1s.

"""

df.head()

df.info()

df.columns

df = df.drop(["CUST_ID"],axis = 1)

df

df.shape

df.duplicated().sum()

"""Q2. perform the following exploratory data analytics tasks:
a. Missing Values Analysis
"""

df=df.dropna()

df.isna().sum()

"""b. Outlier Treatment using the Z-score method

c. Deal with correlated variables
"""

df.corr()

df.corr().style.background_gradient(cmap="gist_heat")

"""
Q3 Perform dimensionality reduction using PCA such that the 95% of the variance is explained"""

x = df.drop(['TENURE'],axis = 1)
y = df['TENURE']

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.10, random_state = 0)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

from sklearn.linear_model import LogisticRegression

model= LogisticRegression()

model.fit(x_train,y_train)

pre=model.predict(x_test)
pre

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
accuracy_score(y_test,pre)

confusion_matrix(y_test,pre)

LR=classification_report(y_test,pre)
print(LR)

"""pca model"""

from sklearn.decomposition import PCA
a=PCA(n_components=0.95)

a.fit(x_train)

xtrain=a.transform(x_train)
print(xtrain.shape)

xtest=a.transform(x_test)
print(xtest.shape)

model2 = LogisticRegression(penalty='l1',solver='liblinear')

model2.fit(xtrain,y_train)

pre2=model2.predict(xtest)

y_test,pre,pre2

accuracy_score_2=accuracy_score(pre2,y_test)
accuracy_score_2

"""Q4. Find the optimum value of k for k-means clustering using 
the elbow method. Plot the elbow curve
"""

from sklearn.cluster import KMeans

k=[]  
for i in range(2,9):
  kmeans=KMeans(n_clusters=i, 
                init='k-means++', 
                n_init=10, 
                max_iter=300, 
                tol=0.0001,  
                verbose=0, 
                random_state=None, 
                copy_x=True,  
                algorithm='auto')
  
  kmeans.fit(df)
  k.append(kmeans.inertia_)

plt.plot(range(2,9),k)

"""Q5. Find the optimum value of k for k-means clustering using 
the silhouette score method and specify the number of 
observations in each cluster using a bar plot.
"""

import sklearn.metrics as metrics
for i in range(3,10):
    kmeans=KMeans(n_clusters=i,
                  init="k-means++",
                  random_state=200)
    kmeans.fit(df)
    labels=kmeans.labels_
    print ("Silhouette score for k(clusters) = "+str(i)+" is "
           +str(metrics.silhouette_score(df,
                                         labels,
                                         metric="euclidean",
                                         sample_size=1000,
                                         random_state=200)))
import matplotlib.pyplot as plt
from yellowbrick.cluster import SilhouetteVisualizer

fig, ax = plt.subplots(2, 2, figsize=(15,8))
for i in [2, 3, 4, 5]:
    '''
    Create KMeans instance for different number of clusters
    '''
    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)
    q, mod = divmod(i, 2)
    '''
    Create SilhouetteVisualizer instance with KMeans instance
    Fit the visualizer
    '''
    visualizer = SilhouetteVisualizer(km, colors='gist_heat', ax=ax[q-1][mod])
    visualizer.fit(x)

"""Part II: Deep Learning"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df2=pd.read_csv("/content/Sentiment.csv")

df2

df2.shape

"""Q1. Print the total number of positive and negative sentiments."""

positive=0
negative=0
for i in df2['sentiment']:
  if i =='Positive':
    positive=positive+1
  elif i=='Negative':
    negative=negative+1
print("Number of positive sentiment :", positive)
print("Number of negative sentiment :", negative)

"""
Q2. Build a sequential LSTM model to predict positive and negative sentiments."""

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

corpus = []
for i in range(0, 13871):
    review = re.sub('[^a-zA-Z]', ' ', 
                    df2['text'][i])
    review = review.lower()
    review = review.split()
    ps = PorterStemmer()
    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
    review = ' '.join(review)
    corpus.append(review)
print(corpus)